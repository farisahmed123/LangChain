{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63830699",
   "metadata": {},
   "source": [
    "# Models Integration with OpenAI, Google Gemini and GROQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30495553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"]=os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENROUTER_API_KEY\"]=os.getenv(\"OPENROUTER_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9e9d69",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unable to infer model provider for model='openrouter/gpt-3.5-turbo'. Please specify 'model_provider' directly.\n\nSupported providers: anthropic, azure_ai, azure_openai, bedrock, bedrock_converse, cohere, deepseek, fireworks, google_anthropic_vertex, google_genai, google_vertexai, groq, huggingface, ibm, mistralai, nvidia, ollama, openai, perplexity, together, upstage, xai\n\nFor help with specific providers, see: https://docs.langchain.com/oss/python/integrations/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mOPENROUTER_API_KEY\u001b[39m\u001b[33m\"\u001b[39m] = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mOPENROUTER_API_KEY\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")  # Not set, so commented out\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m models = \u001b[43minit_chat_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenrouter/gpt-3.5-turbo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Example for OpenRouter\u001b[39;00m\n\u001b[32m     12\u001b[39m models\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Antigravity\\langchain\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:452\u001b[39m, in \u001b[36minit_chat_model\u001b[39m\u001b[34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[39m\n\u001b[32m    444\u001b[39m     warnings.warn(\n\u001b[32m    445\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m has been set but no fields are configurable. Set \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    447\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mconfigurable.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    448\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    449\u001b[39m     )\n\u001b[32m    451\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_init_chat_model_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[32m    458\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m] = model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Antigravity\\langchain\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:474\u001b[39m, in \u001b[36m_init_chat_model_helper\u001b[39m\u001b[34m(model, model_provider, **kwargs)\u001b[39m\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_init_chat_model_helper\u001b[39m(\n\u001b[32m    469\u001b[39m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    470\u001b[39m     *,\n\u001b[32m    471\u001b[39m     model_provider: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    472\u001b[39m     **kwargs: Any,\n\u001b[32m    473\u001b[39m ) -> BaseChatModel:\n\u001b[32m--> \u001b[39m\u001b[32m474\u001b[39m     model, model_provider = \u001b[43m_parse_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     creator_func = _get_chat_model_creator(model_provider)\n\u001b[32m    476\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m creator_func(model=model, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Antigravity\\langchain\\.venv\\Lib\\site-packages\\langchain\\chat_models\\base.py:570\u001b[39m, in \u001b[36m_parse_model\u001b[39m\u001b[34m(model, model_provider)\u001b[39m\n\u001b[32m    562\u001b[39m     supported_list = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28msorted\u001b[39m(_SUPPORTED_PROVIDERS))\n\u001b[32m    563\u001b[39m     msg = (\n\u001b[32m    564\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to infer model provider for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    565\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease specify \u001b[39m\u001b[33m'\u001b[39m\u001b[33mmodel_provider\u001b[39m\u001b[33m'\u001b[39m\u001b[33m directly.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    568\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://docs.langchain.com/oss/python/integrations/providers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    569\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    572\u001b[39m \u001b[38;5;66;03m# Normalize provider name\u001b[39;00m\n\u001b[32m    573\u001b[39m model_provider = model_provider.replace(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m).lower()\n",
      "\u001b[31mValueError\u001b[39m: Unable to infer model provider for model='openrouter/gpt-3.5-turbo'. Please specify 'model_provider' directly.\n\nSupported providers: anthropic, azure_ai, azure_openai, bedrock, bedrock_converse, cohere, deepseek, fireworks, google_anthropic_vertex, google_genai, google_vertexai, groq, huggingface, ibm, mistralai, nvidia, ollama, openai, perplexity, together, upstage, xai\n\nFor help with specific providers, see: https://docs.langchain.com/oss/python/integrations/providers"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")  # Not set, so commented out\n",
    "\n",
    "# Example for OpenRouter: specify model_provider explicitly\n",
    "models = init_chat_model(\"gpt-3.5-turbo\", model_provider=\"openrouter\")\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e803db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
